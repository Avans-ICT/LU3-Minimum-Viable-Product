{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOVCm_Q3b1DN",
    "outputId": "eee040bd-a4fa-4b85-f8c9-e8a15d639ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\serin\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\serin\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\serin\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\serin\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiebiHUpbrya",
    "outputId": "3e9b5172-4228-4aa0-8097-71c4ad903047",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\serin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\serin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\serin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Dit is een voorbeeld tekst die ik zo meteen ga gebruiken. Ik ga nu een paar getallen toevoegen: 1, 2, 3 en ik ga nog wat tekens toevoegen: @ # *\n"
     ]
    }
   ],
   "source": [
    "# 1. Originele tekst\n",
    "text = \"Dit is een voorbeeld tekst die ik zo meteen ga gebruiken. Ik ga nu een paar getallen toevoegen: 1, 2, 3 en ik ga nog wat tekens toevoegen: @ # *\"\n",
    "\n",
    "print(\"Original Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowered text:\n",
      " dit is een voorbeeld tekst die ik zo meteen ga gebruiken. ik ga nu een paar getallen toevoegen: 1, 2, 3 en ik ga nog wat tekens toevoegen: @ # *\n"
     ]
    }
   ],
   "source": [
    "# 2. Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(\"\\nLowered text:\\n\", text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text without numbers:\n",
      " dit is een voorbeeld tekst die ik zo meteen ga gebruiken. ik ga nu een paar getallen toevoegen: , ,  en ik ga nog wat tekens toevoegen: @ # *\n"
     ]
    }
   ],
   "source": [
    "# 3. Cijfers verwijderen\n",
    "text_no_numbers = re.sub(r'\\d+', '', text_lower)\n",
    "print(\"\\nText without numbers:\\n\", text_no_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text without punctuation:\n",
      " dit is een voorbeeld tekst die ik zo meteen ga gebruiken  ik ga nu een paar getallen toevoegen       en ik ga nog wat tekens toevoegen       \n"
     ]
    }
   ],
   "source": [
    "# 4. Leestekens verwijderen\n",
    "text_no_punctuation = re.sub(r'[^\\w\\s]', ' ', text_no_numbers)\n",
    "print(\"\\nText without punctuation:\\n\", text_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Words:\n",
      " ['dit', 'is', 'een', 'voorbeeld', 'tekst', 'die', 'ik', 'zo', 'meteen', 'ga', 'gebruiken', 'ik', 'ga', 'nu', 'een', 'paar', 'getallen', 'toevoegen', 'en', 'ik', 'ga', 'nog', 'wat', 'tekens', 'toevoegen']\n"
     ]
    }
   ],
   "source": [
    "# 5. Tokenization (woorden)\n",
    "words = word_tokenize(text_no_punctuation, language='dutch')\n",
    "print(\"\\nTokenized Words:\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words after Stopword Removal:\n",
      " ['voorbeeld', 'tekst', 'meteen', 'ga', 'gebruiken', 'ga', 'paar', 'getallen', 'toevoegen', 'ga', 'tekens', 'toevoegen']\n"
     ]
    }
   ],
   "source": [
    "# 6. Stopwoorden verwijderen\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "words_no_stop = [w for w in words if w not in stop_words]\n",
    "print(\"\\nWords after Stopword Removal:\\n\", words_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Words:\n",
      " ['voorbeeld', 'tekst', 'meteen', 'ga', 'gebruiken', 'ga', 'paar', 'getallen', 'toevoegen', 'ga', 'tekens', 'toevoegen']\n"
     ]
    }
   ],
   "source": [
    "# 7. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_lemmatized = [lemmatizer.lemmatize(w) for w in words_no_stop]\n",
    "print(\"\\nLemmatized Words:\\n\", words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words:\n",
      " ['voorbeeld', 'tekst', 'meteen', 'ga', 'gebruiken', 'ga', 'paar', 'getallen', 'toevoegen', 'ga', 'teken', 'toevoegen']\n"
     ]
    }
   ],
   "source": [
    "# 8. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "words_stemmed = [stemmer.stem(w) for w in words_no_stop]\n",
    "print(\"\\nStemmed Words:\\n\", words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text:\n",
      " voorbeeld tekst meteen ga gebruiken ga paar getallen toevoegen ga tekens toevoegen\n"
     ]
    }
   ],
   "source": [
    "# 9. Schoon gemaakte tekst\n",
    "clean_text = \" \".join(words_lemmatized)\n",
    "print(\"\\nCleaned Text:\\n\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflectie stap 1\n",
    "\n",
    "- Welke woorden verdwenen tijdens het schoonmaken?\n",
    "\n",
    "Woorden die vaak voorkomen zoals \"dit, is, een, die, ik, ga, nu, en\" zijn verwijderd als stopwoorden. Ook cijfers en speciale tekens zijn weggehaald.\n",
    "\n",
    "- Waarom is dat nuttig bij NLP?\n",
    "\n",
    "Deze woorden en tekens dragen weinig bij aan de betekenis van de zin. Door ze te verwijderen, wordt de tekst korter en wordt het model minder afgeleid door overbodige woorden.\n",
    "\n",
    "- Welke informatie kan eventueel verloren gaan?\n",
    "\n",
    "Soms bevatten stopwoorden toch betekenis, of geven cijfers belangrijke informatie. Door die te verwijderen kan context verloren gaan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " ['voorbeeld', 'tekst', 'meteen', 'ga', 'gebruiken', 'ga', 'paar', 'getallen', 'toevoegen', 'ga', 'tekens', 'toevoegen']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# Word tokenization op de schoongemaakte tekst\n",
    "tokens = word_tokenize(clean_text, language='dutch')\n",
    "print(\"Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aantal tokens: 12\n",
      "Aantal unieke woorden: 9\n"
     ]
    }
   ],
   "source": [
    "# Aantal unieke woorden\n",
    "unique_words = set(tokens)\n",
    "print(\"\\nAantal tokens:\", len(tokens))\n",
    "print(\"Aantal unieke woorden:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequentietabel:\n",
      " Counter({'ga': 3, 'toevoegen': 2, 'voorbeeld': 1, 'tekst': 1, 'meteen': 1, 'gebruiken': 1, 'paar': 1, 'getallen': 1, 'tekens': 1})\n"
     ]
    }
   ],
   "source": [
    "# Frequentietabel\n",
    "freq_table = Counter(tokens)\n",
    "print(\"\\nFrequentietabel:\\n\", freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zinnen (sentence tokenization op originele tekst):\n",
      "Zin 1: Dit is een voorbeeld tekst die ik zo meteen ga gebruiken.\n",
      "Zin 2: Ik ga nu een paar getallen toevoegen: 1, 2, 3 en ik ga nog wat tekens toevoegen: @ # *\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization op de originele tekst\n",
    "sentences = sent_tokenize(text, language='dutch')\n",
    "print(\"\\nZinnen (sentence tokenization op originele tekst):\")\n",
    "for i, s in enumerate(sentences, start=1):\n",
    "    print(f\"Zin {i}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ga</th>\n",
       "      <th>gebruiken</th>\n",
       "      <th>getallen</th>\n",
       "      <th>meteen</th>\n",
       "      <th>paar</th>\n",
       "      <th>tekens</th>\n",
       "      <th>tekst</th>\n",
       "      <th>toevoegen</th>\n",
       "      <th>voorbeeld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ga  gebruiken  getallen  meteen  paar  tekens  tekst  toevoegen  voorbeeld\n",
       "0   1          1         1       1     1       1      1          1          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [clean_text]\n",
    "\n",
    "# One Hot Encoding\n",
    "cv_onehot = CountVectorizer(binary=True)\n",
    "X_onehot = cv_onehot.fit_transform(documents)\n",
    "\n",
    "onehot_df = pd.DataFrame(X_onehot.toarray(),\n",
    "                         columns=cv_onehot.get_feature_names_out())\n",
    "print(\"One Hot Encoding matrix:\")\n",
    "display(onehot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ga</th>\n",
       "      <th>gebruiken</th>\n",
       "      <th>getallen</th>\n",
       "      <th>meteen</th>\n",
       "      <th>paar</th>\n",
       "      <th>tekens</th>\n",
       "      <th>tekst</th>\n",
       "      <th>toevoegen</th>\n",
       "      <th>voorbeeld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ga  gebruiken  getallen  meteen  paar  tekens  tekst  toevoegen  voorbeeld\n",
       "0   3          1         1       1     1       1      1          2          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "cv_bow = CountVectorizer(binary=False)\n",
    "X_bow = cv_bow.fit_transform(documents)\n",
    "\n",
    "bow_df = pd.DataFrame(X_bow.toarray(),\n",
    "                      columns=cv_bow.get_feature_names_out())\n",
    "print(\"\\nBag of Words matrix:\")\n",
    "display(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflectie bij stap 3\n",
    "\n",
    "- Verschil tussen One Hot Encoding en Bag of Words?\n",
    "\n",
    "One Hot Encoding slaat alleen op of een woord in een document voorkomt op een binaire manier.\n",
    "\n",
    "Bag of Words slaat op hoe vaak een woord voorkomt.\n",
    "\n",
    "- Wanneer gebruik je welke?\n",
    "\n",
    "One Hot is voor als je alleen aanwezigheid/afwezigheid wilt, bijvoorbeeld bij heel simpele classificatieproblemen.\n",
    "\n",
    "Bag of Words is nuttig wanneer het belangrijk is hoevaak een woord voorkomt, bijvoorbeeld om te zien welke woorden typerend zijn voor een document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Representatie      | Houdt volgorde van woorden? | Gebruikt frequentie? | Interpretatie                                                                 |\n",
    "|--------------------|-----------------------------|----------------------|-------------------------------------------------------------------------------|\n",
    "| One Hot Encoding   | Nee                         | Nee (alleen 0/1)     | Je ziet alleen of een woord voorkomt in de tekst, niet hoe vaak of waar.     |\n",
    "| Bag of Words       | Nee                         | Ja                   | Je ziet hoe vaak elk woord voorkomt, maar niet de volgorde in de tekst.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Waarom moet tekst eerst worden genormaliseerd vóórdat we modellen trainen?\n",
    "\n",
    "    Omdat ruwe tekst veel variatie en onnodige woorden bevat (hoofdletters, leestekens, meervouden, stopwoorden). Normalisatie maakt de data consistenter, vermindert de dimensie van je feature-ruimte en helpt modellen om patronen beter te herkennen.\n",
    "\n",
    "2. Wat is het verschil tussen NLP, NLU en NLG?\n",
    "\n",
    "    - NLP (Natural Language Processing): alle technieken om natuurlijke taal te verwerken (tokenization, tagging, parsing, etc.).\n",
    "\n",
    "    - NLU (Natural Language Understanding): onderdeel van NLP dat zich richt op begrijpen van betekenis, intenties en relaties in tekst.\n",
    "\n",
    "    - NLG (Natural Language Generation): het automatisch genereren van natuurlijke taal, zoals tekst of antwoorden.\n",
    "\n",
    "\n",
    "3. Waar zou je NLP in het echte leven kunnen toepassen?\n",
    "\n",
    "    Bijvoorbeeld bij spamfilters, automatische vertaling, chatbots, sentimentanalyse op reviews, zoekmachines, spraakassistenten, het automatisch samenvatten van teksten, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
